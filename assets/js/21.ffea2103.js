(window.webpackJsonp=window.webpackJsonp||[]).push([[21],{609:function(e,_,a){e.exports=a.p+"assets/img/image-20211213172922809.b42a034c.png"},610:function(e,_,a){e.exports=a.p+"assets/img/image-20211213151022158.33373307.png"},611:function(e,_,a){e.exports=a.p+"assets/img/image-20211213172845338.737dc6a1.png"},612:function(e,_,a){e.exports=a.p+"assets/img/image-20211213172905426.a63ba7d4.png"},613:function(e,_,a){e.exports=a.p+"assets/img/image-20211213171750441.ca4fc7d6.png"},771:function(e,_,a){"use strict";a.r(_);var s=a(31),t=Object(s.a)({},(function(){var e=this,_=e.$createElement,s=e._self._c||_;return s("ContentSlotsDistributor",{attrs:{"slot-key":e.$parent.slotKey}},[s("h1",{attrs:{id:"pixel2mesh-generating-3d-mesh-models-from-single-rgb-images"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#pixel2mesh-generating-3d-mesh-models-from-single-rgb-images"}},[e._v("#")]),e._v(" Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images")]),e._v(" "),s("p",[e._v("ECCV 2018，代码开放")]),e._v(" "),s("p",[e._v("Github："),s("a",{attrs:{href:"https://github.com/nywang16/Pixel2Mesh",target:"_blank",rel:"noopener noreferrer"}},[e._v("nywang16/Pixel2Mesh: Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. In ECCV2018. (github.com)"),s("OutboundLink")],1)]),e._v(" "),s("p",[e._v("笔记参考：")]),e._v(" "),s("blockquote",[s("ol",[s("li",[s("a",{attrs:{href:"https://www.jianshu.com/p/6c4bac018f79",target:"_blank",rel:"noopener noreferrer"}},[e._v("论文 | Pixel2Mesh三维重建模型解读《Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images》 - 简书 (jianshu.com)"),s("OutboundLink")],1)])])]),e._v(" "),s("h2",{attrs:{id:"_3-本文模型方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-本文模型方法"}},[e._v("#")]),e._v(" 3 本文模型方法")]),e._v(" "),s("h3",{attrs:{id:"_3-1-主要模型方法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-1-主要模型方法"}},[e._v("#")]),e._v(" 3.1 主要模型方法")]),e._v(" "),s("p",[s("img",{attrs:{src:a(609),alt:"image-20211213172922809"}})]),e._v(" "),s("ol",[s("li",[e._v("给定一张输入图像：Input image")]),e._v(" "),s("li",[e._v("为任意的输入图像都初始化一个椭球体作为其初始三维形状：Ellipsoid Mesh")]),e._v(" "),s("li",[e._v("整个网络可以大概分成上下两个部分；\n"),s("ul",[s("li",[e._v("上面部分负责用全卷积神经网络提取输入图像的特征；")]),e._v(" "),s("li",[e._v("下面部分负责用图卷积神经网络来表示三维mesh，并对三维mesh不断进行形变，目标是得到最终的输出（最后边的飞机）")])])]),e._v(" "),s("li",[e._v("注意到图中的"),s("code",[e._v("perceptual feature pooling")]),e._v("层将上面的2D图像信息和下面的3Dmesh信息联系在一起了，即通过借鉴2D图像特征来调整3D mesh中的图卷积网络的节点状态。这个过程可以看成是"),s("code",[e._v("mesh deformation")]),e._v(".")]),e._v(" "),s("li",[e._v("细心的同学应该也注意到除了刚刚提到的"),s("code",[e._v("mesh deformation")]),e._v("，下面这部分还有一个很关键的组成是"),s("code",[e._v("graph uppooling")]),e._v("。文章提出这个图上采样层是为了让图节点依次增加，从图中可以直接看到节点数是由156--\x3e628--\x3e2466变换的，这其实就是coarse-to-fine的体现。")])]),e._v(" "),s("h3",{attrs:{id:"_3-2-各个关键部分"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-各个关键部分"}},[e._v("#")]),e._v(" 3.2 各个关键部分")]),e._v(" "),s("h4",{attrs:{id:"_3-2-1-图卷积神经网络gcn"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-2-1-图卷积神经网络gcn"}},[e._v("#")]),e._v(" 3.2.1 图卷积神经网络GCN")]),e._v(" "),s("p",[e._v("图卷积神经网络的定义如下：")]),e._v(" "),s("img",{staticStyle:{zoom:"67%"},attrs:{src:a(610),alt:"image-20211213151022158"}}),e._v(" "),s("p",[e._v("其中：")]),e._v(" "),s("ul",[s("li",[e._v("f(p,l), f(p,l+1)分别表示顶点p在卷积操作前后的特征向量；")]),e._v(" "),s("li",[e._v("N(p)指顶点p的邻居节点；")]),e._v(" "),s("li",[e._v("W1,W2表示待学习的参数；")])]),e._v(" "),s("p",[e._v("其实整个公式就是表达了图卷积神经网络的"),s("strong",[e._v("节点")]),e._v("是根据其自身的特征和邻居节点的特征来进行更新的。")]),e._v(" "),s("p",[e._v("这样就解决了3D mesh的表示问题以及如何更新节点状态的问题。")]),e._v(" "),s("p",[e._v("3.2.2 融合2D和3D信息")]),e._v(" "),s("p",[e._v("perceptual feature pooling & mesh deformation block")]),e._v(" "),s("p",[e._v("文中用了经典的VGG网络来提取二维的图像信息，而用GCN来表示3D mesh，那么如何在两个不同的模态的数据之间融合工作，更好的利用2D的图像信息来帮助重建3D mesh")]),e._v(" "),s("p",[s("img",{attrs:{src:a(611),alt:"image-20211213172845338"}})]),e._v(" "),s("p",[s("strong",[e._v("mesh deformation block")])]),e._v(" "),s("ol",[s("li",[s("p",[e._v("如上图(a)所示。")])]),e._v(" "),s("li",[s("p",[e._v("C表示三维顶点坐标，P表示图像特征，F表示三维顶点特征；")])]),e._v(" "),s("li",[s("p",[s("code",[e._v("perceptual feature pooling")]),e._v("层负责根据三维顶点坐标C(i-1)去图像特征P中提取对应的信息；")])]),e._v(" "),s("li",[s("p",[e._v("以上提取到的各个顶点特征再与上一时刻的顶点特征F(i-1)做融合，作为G-ResNet的输入；")])]),e._v(" "),s("li",[s("p",[e._v("G-ResNet(graph-based ResNet)产生的输出又做为mesh deformable block的输出，得到新的三维坐标C(i)和三维顶点特征F(i)。")])])]),e._v(" "),s("p",[s("strong",[e._v("perceptural feature pooling")])]),e._v(" "),s("ol",[s("li",[e._v("如上图(b)所示。")]),e._v(" "),s("li",[e._v("负责给定三维坐标点以及图像特征的情况下，获取到三维点对应的特征信息。")]),e._v(" "),s("li",[e._v("首先将3D坐标信息映射回2D坐标点；")]),e._v(" "),s("li",[e._v("取2D坐标点边上最近的四个点进行双线性插值，其结果做为这个顶点的特征；")]),e._v(" "),s("li",[e._v("特别的文章中取了VGG中的"),s("code",[e._v("conv3_3")]),e._v("（256维）,"),s("code",[e._v("conv4_3")]),e._v("（512维）,"),s("code",[e._v("conv_5_3")]),e._v("（512维）的特征进行连接，那么每个顶点就有1280维的特征。")]),e._v(" "),s("li",[e._v("除了最开始的block没有F(i-1)的信息外，其他的block都还能利用上一时刻的128维度的F信息，一共1408维。")])]),e._v(" "),s("p",[s("strong",[e._v("G-ResNet")])]),e._v(" "),s("ol",[s("li",[e._v("如上图(a)中间的部分。")]),e._v(" "),s("li",[e._v("前面为每个顶点都得到了1408维的特征（除了第一个block）通过G-ResNet就能得到新的位置坐标C和每个顶点的形状特征F；")]),e._v(" "),s("li",[e._v("这就需要节点之间有效的信息交换，但每次图卷积网络只能交换邻居节点的信息，很影响新的交换效率，有点类似2D CNN的小感受野。所以增加了shortcut结构。")]),e._v(" "),s("li",[e._v("每个block的G-ResNet的结构都是一样的（14个conv + 1 shortcut），输出128维，这样就产生新的128维的节点形状信息。")])]),e._v(" "),s("h4",{attrs:{id:"_3-3-3-图上采样-graph-upooling"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-3-图上采样-graph-upooling"}},[e._v("#")]),e._v(" 3.3.3 图上采样（Graph upooling）")]),e._v(" "),s("ol",[s("li",[e._v("主要是为了节点数量能够逐渐增加，降低训练难度；")]),e._v(" "),s("li",[e._v("主要有face-based和edge-based这2种方式，都比较好理解，就不再解释了；")]),e._v(" "),s("li",[e._v("文章中采用的edge-based的方式。")])]),e._v(" "),s("p",[s("img",{attrs:{src:a(612),alt:"image-20211213172905426"}})]),e._v(" "),s("h4",{attrs:{id:"_3-3-4-损失函数-losses"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_3-3-4-损失函数-losses"}},[e._v("#")]),e._v(" 3.3.4 损失函数 losses")]),e._v(" "),s("p",[e._v("文章一共为网络模型设计了4种不同的loss，来从不同角度保证网络模型的性能。")]),e._v(" "),s("ol",[s("li",[e._v("**Chamfer 损失函数：**其作用是限制网格顶点的具体位置；（倒角）")]),e._v(" "),s("li",[e._v("**Normal 损失函数：**其作用是增强网格表面法向的一致性，增加表面光滑度；（表面法向损失）")]),e._v(" "),s("li",[e._v("**Laplacian 正则化：**其作用是在形变时维持临近顶点的相对位置；（拉普拉斯损失）")]),e._v(" "),s("li",[e._v("**Edge length 正则化：**其作用是防止个别异常顶点的出现。（边缘损失）")])]),e._v(" "),s("h2",{attrs:{id:"_4-实验"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-实验"}},[e._v("#")]),e._v(" 4 实验")]),e._v(" "),s("h3",{attrs:{id:"_4-1-实验设置"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-1-实验设置"}},[e._v("#")]),e._v(" 4.1 实验设置")]),e._v(" "),s("p",[e._v("数据，评价指标，对比标准，训练和测试，...")]),e._v(" "),s("h3",{attrs:{id:"_4-2-与最新技术的对比"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-2-与最新技术的对比"}},[e._v("#")]),e._v(" 4.2 与最新技术的对比")]),e._v(" "),s("h3",{attrs:{id:"_4-3-消融实验"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#_4-3-消融实验"}},[e._v("#")]),e._v(" 4.3 消融实验")]),e._v(" "),s("p",[e._v("消融研究的定性结果。该图真实地反映了各组成部分的贡献，尤其是正则化组成部分。")]),e._v(" "),s("p",[s("img",{attrs:{src:a(613),alt:"image-20211213171750441"}})])])}),[],!1,null,null,null);_.default=t.exports}}]);