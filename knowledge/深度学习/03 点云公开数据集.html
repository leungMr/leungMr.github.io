<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>点云公开数据集 | 阿信 _notes</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.fd010194.css" as="style"><link rel="preload" href="/assets/js/app.4da1065c.js" as="script"><link rel="preload" href="/assets/js/3.d419b5b7.js" as="script"><link rel="preload" href="/assets/js/8.c64fe8ce.js" as="script"><link rel="prefetch" href="/assets/js/10.4bad6486.js"><link rel="prefetch" href="/assets/js/11.3dcdeb34.js"><link rel="prefetch" href="/assets/js/12.4bed0e42.js"><link rel="prefetch" href="/assets/js/13.b6a43f3c.js"><link rel="prefetch" href="/assets/js/14.3f3df886.js"><link rel="prefetch" href="/assets/js/15.1266ea8b.js"><link rel="prefetch" href="/assets/js/16.bffd9bb8.js"><link rel="prefetch" href="/assets/js/17.97b56ab2.js"><link rel="prefetch" href="/assets/js/18.44f604f8.js"><link rel="prefetch" href="/assets/js/19.a02c01a8.js"><link rel="prefetch" href="/assets/js/2.5d29b4a8.js"><link rel="prefetch" href="/assets/js/20.022f3328.js"><link rel="prefetch" href="/assets/js/21.ffea2103.js"><link rel="prefetch" href="/assets/js/22.7ea6fe99.js"><link rel="prefetch" href="/assets/js/23.acf69f2c.js"><link rel="prefetch" href="/assets/js/24.6f720be1.js"><link rel="prefetch" href="/assets/js/25.e5f77a77.js"><link rel="prefetch" href="/assets/js/26.2789ae5a.js"><link rel="prefetch" href="/assets/js/27.2e549eb0.js"><link rel="prefetch" href="/assets/js/28.e17e02be.js"><link rel="prefetch" href="/assets/js/29.67e72e71.js"><link rel="prefetch" href="/assets/js/30.e3dda54c.js"><link rel="prefetch" href="/assets/js/31.210574c3.js"><link rel="prefetch" href="/assets/js/32.6ba63bb2.js"><link rel="prefetch" href="/assets/js/33.45d07bfa.js"><link rel="prefetch" href="/assets/js/34.3281ef49.js"><link rel="prefetch" href="/assets/js/35.b634cff1.js"><link rel="prefetch" href="/assets/js/36.491b73c3.js"><link rel="prefetch" href="/assets/js/37.e35dba7c.js"><link rel="prefetch" href="/assets/js/38.0756e011.js"><link rel="prefetch" href="/assets/js/39.675f9f54.js"><link rel="prefetch" href="/assets/js/4.a4009e16.js"><link rel="prefetch" href="/assets/js/40.8fe858a4.js"><link rel="prefetch" href="/assets/js/41.81a5670f.js"><link rel="prefetch" href="/assets/js/42.83bf3204.js"><link rel="prefetch" href="/assets/js/43.c724dca3.js"><link rel="prefetch" href="/assets/js/44.8513f2c9.js"><link rel="prefetch" href="/assets/js/45.3a590740.js"><link rel="prefetch" href="/assets/js/46.4854a550.js"><link rel="prefetch" href="/assets/js/47.80212a21.js"><link rel="prefetch" href="/assets/js/48.1a83df73.js"><link rel="prefetch" href="/assets/js/49.aa599a69.js"><link rel="prefetch" href="/assets/js/5.bf342ddc.js"><link rel="prefetch" href="/assets/js/50.7b9ab880.js"><link rel="prefetch" href="/assets/js/51.8e16855c.js"><link rel="prefetch" href="/assets/js/52.8e6730bc.js"><link rel="prefetch" href="/assets/js/53.f2ab5058.js"><link rel="prefetch" href="/assets/js/54.cbc29320.js"><link rel="prefetch" href="/assets/js/55.c04f43e3.js"><link rel="prefetch" href="/assets/js/56.b097240f.js"><link rel="prefetch" href="/assets/js/57.f09ce6f9.js"><link rel="prefetch" href="/assets/js/58.0526911b.js"><link rel="prefetch" href="/assets/js/59.61d88944.js"><link rel="prefetch" href="/assets/js/6.9b7db570.js"><link rel="prefetch" href="/assets/js/60.1a1f0c25.js"><link rel="prefetch" href="/assets/js/61.328ea412.js"><link rel="prefetch" href="/assets/js/62.21cd989b.js"><link rel="prefetch" href="/assets/js/63.62a5ea56.js"><link rel="prefetch" href="/assets/js/64.e46e59c0.js"><link rel="prefetch" href="/assets/js/65.a9ceebdb.js"><link rel="prefetch" href="/assets/js/66.3de1336a.js"><link rel="prefetch" href="/assets/js/67.2da4f992.js"><link rel="prefetch" href="/assets/js/68.1e8182b6.js"><link rel="prefetch" href="/assets/js/69.37a87946.js"><link rel="prefetch" href="/assets/js/7.2571df95.js"><link rel="prefetch" href="/assets/js/70.659b256f.js"><link rel="prefetch" href="/assets/js/71.bb371526.js"><link rel="prefetch" href="/assets/js/72.b8db4a6c.js"><link rel="prefetch" href="/assets/js/73.3cd8b5b0.js"><link rel="prefetch" href="/assets/js/74.7f877c14.js"><link rel="prefetch" href="/assets/js/75.f7a92900.js"><link rel="prefetch" href="/assets/js/76.4de41963.js"><link rel="prefetch" href="/assets/js/77.ef073569.js"><link rel="prefetch" href="/assets/js/9.57cea2e1.js">
    <link rel="stylesheet" href="/assets/css/0.styles.fd010194.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">阿信 _notes</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/paper/" class="nav-link">
  文献阅读
</a></div><div class="nav-item"><a href="/knowledge/" class="nav-link router-link-active">
  零碎知识
</a></div><div class="nav-item"><a href="/work/" class="nav-link">
  日常工作
</a></div><div class="nav-item"><a href="/code/" class="nav-link">
  编程开发
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/paper/" class="nav-link">
  文献阅读
</a></div><div class="nav-item"><a href="/knowledge/" class="nav-link router-link-active">
  零碎知识
</a></div><div class="nav-item"><a href="/work/" class="nav-link">
  日常工作
</a></div><div class="nav-item"><a href="/code/" class="nav-link">
  编程开发
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>点云理论</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>深度学习</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/knowledge/深度学习/01 入门神经网络基础.html" class="sidebar-link">一天入门神经网络基础</a></li><li><a href="/knowledge/深度学习/02 遥感影像深度学习.html" class="sidebar-link">遥感影像深度学习</a></li><li><a href="/knowledge/深度学习/03 点云公开数据集.html" class="active sidebar-link">点云公开数据集</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-点云分类和分割" class="sidebar-link">1 点云分类和分割</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-1-modelnet40-点云分类" class="sidebar-link">1.1 ModelNet40 （点云分类）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-2-shapenet-部件分割" class="sidebar-link">1.2  ShapeNet （部件分割）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-3-partnet-部件分割" class="sidebar-link">1.3 PartNet （部件分割）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-4-s3dis-室内场景、语义分割" class="sidebar-link">1.4 S3DIS （室内场景、语义分割）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-5-scannet-室内场景、语义标签" class="sidebar-link">1.5 Scannet（室内场景、语义标签）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-6-semantic3d-室外自然场景-语义分割" class="sidebar-link">1.6  Semantic3D（室外自然场景，语义分割）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-7-semantic-kitti-道路点云-语义分割" class="sidebar-link">1.7  Semantic KITTI （道路点云，语义分割）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_1-8-whu-tls-mls-点云配准、语义分割、实例分割" class="sidebar-link">1.8 WHU-TLS/MLS （点云配准、语义分割、实例分割）</a></li></ul></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_2-点云配准" class="sidebar-link">2 点云配准</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_2-1-the-stanford-3d-scanning-repository-点云配准、表面重建" class="sidebar-link">2.1 The Stanford 3D Scanning Repository （点云配准、表面重建）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_2-2-3d-match-点云配准" class="sidebar-link">2.2 3D Match （点云配准）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_2-3-asl-datasets-repository-点云配准、目标检测" class="sidebar-link">2.3 ASL Datasets Repository  (点云配准、目标检测)</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_2-4-sydney-urban-objects-dataset-点云配准、点云分类" class="sidebar-link">2.4 Sydney Urban Objects Dataset （点云配准、点云分类）</a></li></ul></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_3-目标检测-驾驶场景" class="sidebar-link">3 目标检测（驾驶场景）</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_3-1-kitti-目标检测" class="sidebar-link">3.1 KITTI （目标检测）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_3-2-iqmulus-terramobilita-contest-检测、分类、分割" class="sidebar-link">3.2 IQmulus &amp; TerraMobilita Contest （检测、分类、分割）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_3-3-nuscenes-检测、追踪、分割" class="sidebar-link">3.3 nuScenes （检测、追踪、分割）</a></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_3-4-waymo-open-dataset-检测、追踪" class="sidebar-link">3.4 Waymo Open Dataset （检测、追踪）</a></li></ul></li><li class="sidebar-sub-header"><a href="/knowledge/深度学习/03 点云公开数据集.html#_4-机载点云数据集" class="sidebar-link">4 机载点云数据集</a></li></ul></li><li><a href="/knowledge/深度学习/04 2017 PointNet PointNet++.html" class="sidebar-link">PointNet PointNet++</a></li><li><a href="/knowledge/深度学习/04 2018 PointCNN.html" class="sidebar-link">PointCNN</a></li><li><a href="/knowledge/深度学习/04 2019 KPConv.html" class="sidebar-link">KPConv</a></li><li><a href="/knowledge/深度学习/04 2020 PCT.html" class="sidebar-link">/knowledge/深度学习/04 2020 PCT.html</a></li><li><a href="/knowledge/深度学习/04 2020 RandLA-Net.html" class="sidebar-link">RandLA-Net: Efficient Semantic Segmentation of Large-Scale Point Clouds</a></li><li><a href="/knowledge/深度学习/04 2022 PointMLP.html" class="sidebar-link">/knowledge/深度学习/04 2022 PointMLP.html</a></li><li><a href="/knowledge/深度学习/05 机载Lidar深度学习.html" class="sidebar-link">思维瞬间：</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>专业储备</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="点云公开数据集"><a href="#点云公开数据集" class="header-anchor">#</a> 点云公开数据集</h1> <p>作者：gisleung</p> <p>时间：2022年5月</p> <h2 id="_1-点云分类和分割"><a href="#_1-点云分类和分割" class="header-anchor">#</a> 1 点云分类和分割</h2> <h3 id="_1-1-modelnet40-点云分类"><a href="#_1-1-modelnet40-点云分类" class="header-anchor">#</a> 1.1 ModelNet40 （点云分类）</h3> <p>普林斯顿 ModelNet 项目的目标是为计算机视觉、计算机图形学、机器人学和认知科学领域的研究人员提供全面、干净的对象 3D CAD 模型集合。</p> <p><strong>包含数据集：</strong></p> <ul><li><code>ModelNet40</code>：</li> <li><code>ModelNet10</code>：</li> <li><code>modelnet40_ply_hdf5_2048</code>：</li> <li><code>modelnet40_normal_resampled</code></li></ul> <p><strong>数据内容：</strong> 飞机、汽车、椅子、碗、瓶、键盘、楼梯、...</p> <p><strong>采集方式：</strong></p> <p><strong>发布单位：</strong> 普林斯顿大学</p> <p><strong>下载地址：</strong> http://modelnet.cs.princeton.edu，（部分可直接下载，其它需要邮件作者）</p> <p><img src="/assets/img/image-20220517171544004.da21dfd0.png" alt="image-20220517171544004"></p> <hr> <h3 id="_1-2-shapenet-部件分割"><a href="#_1-2-shapenet-部件分割" class="header-anchor">#</a> 1.2  ShapeNet （部件分割）</h3> <p>一个注释丰富的大规模 3D 形状数据集</p> <p><strong>包含数据集：</strong></p> <ul><li><code>ShapeNetCore</code>：ShapeNet数据集的一个子集，干净的3D模型，经过手动验证的分类和注释标签。它涵盖55个常见对象类别，约51300个独特的3D模型。</li> <li><code>ShapeNetSem</code>：一个更小、注释更密集的子集，由12000个模型组成，分布在更广泛的270个类别中。除了手动验证的类别标签和一致的对齐方式外，这些模型还使用真实世界的尺寸标注、类别级别的材料成分估计值以及总体积和重量估计值进行注释。</li></ul> <p><strong>数据内容：</strong> 飞机、汽车、桌子、椅子、电脑、浴缸、...</p> <p><strong>采集方式：</strong></p> <p><strong>发布单位：</strong> 普林斯顿大学，斯坦福大学，TTIC</p> <p><strong>下载地址：</strong> https://shapenet.org/，（需要注册并审核通过）</p> <p><img src="/assets/img/image-20220519114909017.dd7f520d.png" alt="image-20220519114909017"></p> <hr> <h3 id="_1-3-partnet-部件分割"><a href="#_1-3-partnet-部件分割" class="header-anchor">#</a> 1.3 PartNet （部件分割）</h3> <p>一个<em>一致</em>的、<em>大规模</em>的 3D 对象数据集，用<em>细粒度的</em>、<em>实例级</em>的和<em>分层</em>的3D 零件信息进行注释。我们的数据集包含 26,671 个 3D 模型的 573,585 个零件实例，涵盖 24 个对象类别。</p> <p><strong>包含数据集：</strong></p> <ul><li><code>PartNet-v0/v1</code>：语音分割</li> <li><code>PartNet-Symh</code>：使用二元对称层次结构丰富了我们的 PartNet 数据集</li></ul> <p><strong>数据内容：</strong> 桌子、椅子、床、耳机、电脑、...</p> <p><strong>采集方式：</strong></p> <p><strong>发布单位：</strong> 斯坦福大学等</p> <p><strong>下载地址：</strong> https://partnet.cs.stanford.edu/</p> <p><img src="/assets/img/image-20220519113846373.e9f3ae0b.png" alt="image-20220519113846373"></p> <hr> <h3 id="_1-4-s3dis-室内场景、语义分割"><a href="#_1-4-s3dis-室内场景、语义分割" class="header-anchor">#</a> 1.4 S3DIS （室内场景、语义分割）</h3> <p><strong>包含数据集：</strong></p> <ul><li><code>Full 2D-3D-S Dataset</code>：</li> <li><code>S3DIS Dataset</code>：</li></ul> <p><strong>数据内容：</strong> 会议室、个人办公室、礼堂、休息室、开放空间、大厅、楼梯、走廊</p> <p><strong>发布单位：</strong> 斯坦福大学</p> <p><strong>下载地址：</strong> http://buildingparser.stanford.edu/dataset.html （需要登记信息）</p> <p><img src="/assets/img/image-20220517210411450.92ea6cd4.png" alt="image-20220517210411450"></p> <hr> <h3 id="_1-5-scannet-室内场景、语义标签"><a href="#_1-5-scannet-室内场景、语义标签" class="header-anchor">#</a> 1.5 Scannet（室内场景、语义标签）</h3> <p>一个二维三维数据集，主要采集了室内场景的二维图像信息，包括rgb、深度，三维点云ply数据，并进行了语义标签和实例标签标注。</p> <p>一共1513个采集场景数据（每个场景中点云数量都不一样，如果要用到端到端可能需要采样，使每一个场景的点都相同），共21个类别的对象，其中，1201个场景用于训练，312个场景用于测试</p> <p><strong>包含数据集：</strong></p> <ul><li><code>完整数据集</code>：</li> <li><code>子集</code>：</li></ul> <p><strong>数据内容：</strong> 室内场景</p> <p><strong>采集方式：</strong> RGBD摄像机得到的3维激光点云</p> <p><strong>发布单位：</strong> 斯坦福大学，普林斯顿大学，慕尼黑工业大学</p> <p><strong>下载地址：</strong> http://www.scan-net.org/#code-and-data （需要填写协议，邮件作者）</p> <p><img src="/assets/img/image-20220518112013814.b7305455.png" alt="image-20220518112013814"></p> <hr> <h3 id="_1-6-semantic3d-室外自然场景-语义分割"><a href="#_1-6-semantic3d-室外自然场景-语义分割" class="header-anchor">#</a> 1.6  Semantic3D（室外自然场景，语义分割）</h3> <p>一个大型标记的3D点云数据集，其中包含自然场景，总共超过40亿个点。它还涵盖了一系列不同的城市场景：教堂，街道，铁轨，广场，村庄，足球场，城堡等等。我们提供的点云使用最先进的设备进行静态扫描，并包含非常精细的细节。</p> <p><strong>包含数据集：</strong></p> <p><code>semantic-8</code>：具有8个类标签的分类基准，即{1：人造地形，2：自然地形，3：高植被，4：低植被，5：建筑物，6：硬景观，7：扫描文物，8：汽车}。附加标签 {0：未标记的点}</p> <p><code>reduceed-8</code>：缩减版</p> <p><strong>数据内容：</strong> 教堂，街道，铁轨，广场，村庄，足球场，城堡等室外地面扫描结果</p> <p><strong>采集方式：</strong> 室外地面扫描</p> <p><strong>发布单位：</strong> 瑞士苏黎世联邦理工学院</p> <p><strong>下载地址：</strong> http://www.semantic3d.net/ （直接下载）</p> <p><img src="/assets/img/image-20220518111941704.319d8fee.png" alt="image-20220518111941704"></p> <hr> <h3 id="_1-7-semantic-kitti-道路点云-语义分割"><a href="#_1-7-semantic-kitti-道路点云-语义分割" class="header-anchor">#</a> 1.7  Semantic KITTI （道路点云，语义分割）</h3> <p>基于激光雷达序列的语义场景理解数据集，一个基于<code>KITTI视觉基准</code>的大规模数据集，并使用了测距任务提供的所有序列。我们为序列 00-10 的每个单独扫描提供密集注释，从而可以使用多个顺序扫描进行语义场景解释，如语义分割和语义场景完成</p> <p><strong>包含数据集：</strong></p> <ul><li><code>KITTI Odometry Benchmark Velodyne point clouds</code>：</li> <li><code>KITTI Odometry Benchmark calibration data</code>：</li> <li><code>SemanticKITTI label data</code>：</li></ul> <p><strong>数据内容：</strong> 数据集包含10条完整采集轨迹，市中心的交通、住宅区，以及德国卡尔斯鲁厄周围的高速公路场景和乡村道路。共标注 28 个类，包括区分非移动对象和移动对象的类，即地面、建筑、车、人、物体等大类（内部继续细分）。</p> <p><strong>采集方式：</strong> 汽车激光雷达的全360度视场</p> <p><strong>发布单位：</strong> 波恩大学</p> <p><strong>下载地址：</strong>  http://semantic-kitti.org/dataset.html （提供邮箱下载）</p> <p><img src="/assets/img/image-20220518155754277.a0ea2cb2.png" alt="image-20220518155754277"></p> <hr> <h3 id="_1-8-whu-tls-mls-点云配准、语义分割、实例分割"><a href="#_1-8-whu-tls-mls-点云配准、语义分割、实例分割" class="header-anchor">#</a> 1.8 WHU-TLS/MLS （点云配准、语义分割、实例分割）</h3> <p>为推进深度学习方法在点云配准、语义分割、实例分割等领域的发展，武汉大学联合国内外多家高等院校和研究机构发布了包含多类型场景的地面站点云配准基准数据集WHU-TLS和包含语义、实例的城市级车载点云基准数据集WHU-MLS。</p> <p><strong>包含数据集：</strong></p> <ul><li><code>WHU-TLS</code>：地铁站、高铁站、山地、公园、校园、住宅、河岸、文化遗产建筑、地下矿道、隧道等</li> <li><code>WHU-MLS</code>：地面特征，动态目标，植被，杆状地物及其附属结构，建筑和结构设以及其他公共便利设施等6大类30余小类地物要素</li></ul> <p><strong>数据内容：</strong> 数据集涵盖了地铁站、高铁站、山地、森林、公园、校园、住宅、河岸、文化遗产建筑、地下矿道、隧道等11种不同的环境</p> <p><strong>采集方式：</strong> 地面点云、车载点云</p> <p><strong>发布单位：</strong> 武汉大学（联合其它单位）</p> <p><strong>下载地址：</strong> http://3s.whu.edu.cn/ybs/en/benchmark.htm （登记信息下载）</p> <p><img src="/assets/img/image-20220518161915616.c8096b73.png" alt="image-20220518161915616"></p> <h2 id="_2-点云配准"><a href="#_2-点云配准" class="header-anchor">#</a> 2 点云配准</h2> <h3 id="_2-1-the-stanford-3d-scanning-repository-点云配准、表面重建"><a href="#_2-1-the-stanford-3d-scanning-repository-点云配准、表面重建" class="header-anchor">#</a> 2.1 The Stanford 3D Scanning Repository （点云配准、表面重建）</h3> <p><strong>包含数据：</strong> <code>斯坦福模型</code></p> <p><strong>数据内容：</strong> 兔子、龙、马、各类雕像等</p> <p><strong>采集方式：</strong>  Cyberware三维扫描仪</p> <p><strong>发布单位：</strong> 斯坦福大学</p> <p><strong>下载地址：</strong>  http://graphics.stanford.edu/data/3Dscanrep/</p> <p><img src="/assets/img/image-20220518203421918.6712d9c1.png" alt="image-20220518203421918"></p> <hr> <h3 id="_2-2-3d-match-点云配准"><a href="#_2-2-3d-match-点云配准" class="header-anchor">#</a> 2.2 3D Match （点云配准）</h3> <p>3DMatch数据集收集了来⾃于62个场景的数据，其中54个场景的数据⽤于训练，8个场景的数据⽤于评估，其具体名称查看train.txt和test.txt。3DMatch数据常⽤于3D点云的关键点、特征描述⼦、点云配准等任务。</p> <p><strong>包含数据集：</strong> <code>7-Scenes</code>、<code>SUN3D</code> 等</p> <p><strong>数据内容：</strong> 客厅、办公室、厨房、楼梯、等</p> <p><strong>采集方式：</strong>  RGB-D数据生成</p> <p><strong>发布单位：</strong> 普林斯顿大学</p> <p><strong>下载地址：</strong>  https://3dmatch.cs.princeton.edu/  （直接下载）</p> <p><img src="/assets/img/image-20220518194743093.1e9646e0.png" alt="image-20220518194743093"></p> <hr> <h3 id="_2-3-asl-datasets-repository-点云配准、目标检测"><a href="#_2-3-asl-datasets-repository-点云配准、目标检测" class="header-anchor">#</a> 2.3 ASL Datasets Repository  (点云配准、目标检测)</h3> <p>点云配准应用</p> <p><strong>包含数据集：</strong></p> <p><strong>数据内容：</strong> 公寓、楼梯、山地平原、亭子、等</p> <p><strong>采集方式：</strong>  Hokuyo UTM-30LX 激光扫描仪</p> <p><strong>发布单位：</strong> Autonomous Systems Lab （苏黎世联邦理工学院自治系统实验室）</p> <p><strong>下载地址：</strong>   https://projects.asl.ethz.ch/datasets/doku.php （直接下载）</p> <p><img src="/assets/img/image-20220519092652191.2df17141.png" alt="image-20220519092652191"></p> <hr> <h3 id="_2-4-sydney-urban-objects-dataset-点云配准、点云分类"><a href="#_2-4-sydney-urban-objects-dataset-点云配准、点云分类" class="header-anchor">#</a> 2.4 Sydney Urban Objects Dataset （点云配准、点云分类）</h3> <p>包含用Velodyne HDL-64E LIDAR扫描的各种常见<strong>城市道路对象</strong>，收集于澳大利亚悉尼CBD。含有631个单独的扫描物体，包括车辆、行人、广告标志和树木等，可以用来测试匹配和分类算法。</p> <p><strong>包含数据集：</strong> <code>Sydney Urban Objects Dataset</code></p> <p><strong>数据内容：</strong> 城市道路对象：摩托车、汽车、行人、植被、交通标志、等</p> <p><strong>采集方式：</strong>  Velodyne HDL-64E LIDAR，</p> <p><strong>发布单位：</strong> 悉尼大学</p> <p><strong>下载地址：</strong>   https://www.acfr.usyd.edu.au/papers/SydneyUrbanObjectsDataset.shtml （直接下载）</p> <p><img src="/assets/img/image-20220519093819116.4cf30ba3.png" alt="image-20220519093819116"></p> <h2 id="_3-目标检测-驾驶场景"><a href="#_3-目标检测-驾驶场景" class="header-anchor">#</a> 3 目标检测（驾驶场景）</h2> <h3 id="_3-1-kitti-目标检测"><a href="#_3-1-kitti-目标检测" class="header-anchor">#</a> 3.1 KITTI （目标检测）</h3> <p>是目前国际上最大的自动驾驶场景下的算法评测数据集。该数据集用于评测立体图像(stereo)，光流(optical flow)，视觉测距(visual odometry)，3D物体检测(object detection)和3D跟踪(tracking)等计算机视觉技术在车载环境下的性能。</p> <p><strong>包含数据集：</strong> <code>Velodyne point clouds</code>、...</p> <p><strong>数据内容：</strong> 城市、农村、高速公路实时采集</p> <p><strong>采集方式：</strong>  车载Velodyne激光扫描仪</p> <p><strong>发布单位：</strong> 卡尔斯鲁厄理工学院、芝加哥丰田技术研究所</p> <p><strong>下载地址：</strong>   http://www.cvlibs.net/datasets/kitti/ （注册账号下载）</p> <p><img src="/assets/img/image-20220519101440593.9bac2a77.png" alt="image-20220519101440593"></p> <hr> <h3 id="_3-2-iqmulus-terramobilita-contest-检测、分类、分割"><a href="#_3-2-iqmulus-terramobilita-contest-检测、分类、分割" class="header-anchor">#</a> 3.2 IQmulus &amp; TerraMobilita Contest （检测、分类、分割）</h3> <p>该数据库包含由3亿点组成的巴黎密集城市环境的三维MLS数据。在这个数据库中，对整个三维点云进行分割和分类，即每个点包含一个标签和一个类。因此，可用于对检测-分割-分类方法进行点向评估。</p> <p><strong>包含数据集：</strong></p> <ul><li><code>Learning dataset</code> ：用于学习</li> <li><code>ten zones</code>：用于测试</li></ul> <p><strong>数据内容：</strong> 巴黎密集城市环境</p> <p><strong>采集方式：</strong>  车载MLS</p> <p><strong>发布单位：</strong> 法国国家测绘局</p> <p><strong>下载地址：</strong>   http://data.ign.fr/benchmarks/UrbanAnalysis/# （直接下载）</p> <p><img src="/assets/img/image-20220519105204679.e8499a10.png" alt="image-20220519105204679"></p> <hr> <h3 id="_3-3-nuscenes-检测、追踪、分割"><a href="#_3-3-nuscenes-检测、追踪、分割" class="header-anchor">#</a> 3.3 nuScenes （检测、追踪、分割）</h3> <p>包含所有 1000 个场景的完整 nuScenes 数据集。完整数据集包括大约 140 万张摄像机图像、39 万张激光雷达扫描、1.4 米雷达扫描和 4 万个关键帧中的 140 万个对象边界框。2020年7月，发布了nuScenes-lidarseg</p> <p><strong>包含数据集：</strong></p> <ul><li><code>nuScenes</code>：</li> <li><code>nuScenes-lidarseg</code>：激光雷达语义分割</li> <li><code>nuScenes-panoptic</code>：</li></ul> <p><strong>数据内容：</strong> （波士顿和新加坡）密集的交通和极具挑战性的驾驶场景</p> <p><strong>采集方式：</strong>  6个摄像头，1个激光雷达，5个雷达，GPS，IMU</p> <p><strong>发布单位：</strong> motional</p> <p><strong>下载地址：</strong>  https://www.nuscenes.org/nuscenes#data-collection（注册登录下载）</p> <p><img src="/assets/img/image-20220519105833672.2c5a2dff.png" alt="image-20220519105833672"></p> <hr> <h3 id="_3-4-waymo-open-dataset-检测、追踪"><a href="#_3-4-waymo-open-dataset-检测、追踪" class="header-anchor">#</a> 3.4 Waymo Open Dataset （检测、追踪）</h3> <p>数据集包含3000个驾驶片段，每一片段包含20秒的连续驾驶画面。连续镜头内容可以使得研究人员开发模型来跟踪和预测其他道路使用者的行为。数据采集的范围涵盖凤凰城、柯克兰、山景城、旧金山等地区，以及各种驾驶条件下的数据，包括白天、黑夜、黎明、黄昏、雨天和晴天。每个分段涵盖5个高分辨率Waymo激光雷达和五个前置和侧面摄像头的数据</p> <p><strong>包含数据集：</strong></p> <ul><li><code>Perception Dataset</code>：</li> <li><code>Motion Dataset</code>：</li></ul> <p><strong>数据内容：</strong> 包括各种时间、环境，对象和天气条件的驾驶场景</p> <p><strong>采集方式：</strong></p> <ul><li>1 个中档激光雷达</li> <li>4 个短程激光雷达</li> <li>5 个摄像头（正面和侧面）</li> <li>同步激光雷达和摄像头数据</li> <li>激光雷达到摄像机投影</li> <li>传感器校准和车辆姿势</li></ul> <p><strong>发布单位：</strong> Waymo</p> <p><strong>下载地址：</strong>  https://waymo.com/open/ （注册账号下载）</p> <p><img src="/assets/img/image-20220519112040855.39203fff.png" alt="image-20220519112040855"></p> <p><strong>参考（更多资源）：</strong></p> <ul><li><a href="https://zhuanlan.zhihu.com/p/393348763" target="_blank" rel="noopener noreferrer">三维视觉与自动驾驶数据集（40个） - 知乎 (zhihu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://page.om.qq.com/page/OvuLwZCRuhyZVnOwPmHaOLBA0" target="_blank" rel="noopener noreferrer">腾讯内容开放平台 (qq.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://github.com/zhulf0804/3D-PointCloud" target="_blank" rel="noopener noreferrer">zhulf0804/3D-PointCloud: Papers and Datasets about Point Cloud. (github.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://github.com/zhulf0804/3D-PointCloud/blob/master/Datasets.md" target="_blank" rel="noopener noreferrer">3D-PointCloud/Datasets.md at master · zhulf0804/3D-PointCloud (github.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://ieeexplore.ieee.org/abstract/document/9127813" target="_blank" rel="noopener noreferrer">Deep Learning for 3D Point Clouds: A Survey<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul> <h2 id="_4-机载点云数据集"><a href="#_4-机载点云数据集" class="header-anchor">#</a> 4 机载点云数据集</h2> <p>https://zhuanlan.zhihu.com/p/391880971</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/knowledge/深度学习/02 遥感影像深度学习.html" class="prev">
        遥感影像深度学习
      </a></span> <span class="next"><a href="/knowledge/深度学习/04 2017 PointNet PointNet++.html">
        PointNet PointNet++
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.4da1065c.js" defer></script><script src="/assets/js/3.d419b5b7.js" defer></script><script src="/assets/js/8.c64fe8ce.js" defer></script>
  </body>
</html>
