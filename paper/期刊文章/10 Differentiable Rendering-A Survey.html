<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Differentiable Rendering: A Survey | 阿信 _notes</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.fd010194.css" as="style"><link rel="preload" href="/assets/js/app.4da1065c.js" as="script"><link rel="preload" href="/assets/js/3.d419b5b7.js" as="script"><link rel="preload" href="/assets/js/22.7ea6fe99.js" as="script"><link rel="prefetch" href="/assets/js/10.4bad6486.js"><link rel="prefetch" href="/assets/js/11.3dcdeb34.js"><link rel="prefetch" href="/assets/js/12.4bed0e42.js"><link rel="prefetch" href="/assets/js/13.b6a43f3c.js"><link rel="prefetch" href="/assets/js/14.3f3df886.js"><link rel="prefetch" href="/assets/js/15.1266ea8b.js"><link rel="prefetch" href="/assets/js/16.bffd9bb8.js"><link rel="prefetch" href="/assets/js/17.97b56ab2.js"><link rel="prefetch" href="/assets/js/18.44f604f8.js"><link rel="prefetch" href="/assets/js/19.a02c01a8.js"><link rel="prefetch" href="/assets/js/2.5d29b4a8.js"><link rel="prefetch" href="/assets/js/20.022f3328.js"><link rel="prefetch" href="/assets/js/21.ffea2103.js"><link rel="prefetch" href="/assets/js/23.acf69f2c.js"><link rel="prefetch" href="/assets/js/24.6f720be1.js"><link rel="prefetch" href="/assets/js/25.e5f77a77.js"><link rel="prefetch" href="/assets/js/26.2789ae5a.js"><link rel="prefetch" href="/assets/js/27.2e549eb0.js"><link rel="prefetch" href="/assets/js/28.e17e02be.js"><link rel="prefetch" href="/assets/js/29.67e72e71.js"><link rel="prefetch" href="/assets/js/30.e3dda54c.js"><link rel="prefetch" href="/assets/js/31.210574c3.js"><link rel="prefetch" href="/assets/js/32.6ba63bb2.js"><link rel="prefetch" href="/assets/js/33.45d07bfa.js"><link rel="prefetch" href="/assets/js/34.3281ef49.js"><link rel="prefetch" href="/assets/js/35.b634cff1.js"><link rel="prefetch" href="/assets/js/36.491b73c3.js"><link rel="prefetch" href="/assets/js/37.e35dba7c.js"><link rel="prefetch" href="/assets/js/38.0756e011.js"><link rel="prefetch" href="/assets/js/39.675f9f54.js"><link rel="prefetch" href="/assets/js/4.a4009e16.js"><link rel="prefetch" href="/assets/js/40.8fe858a4.js"><link rel="prefetch" href="/assets/js/41.81a5670f.js"><link rel="prefetch" href="/assets/js/42.83bf3204.js"><link rel="prefetch" href="/assets/js/43.c724dca3.js"><link rel="prefetch" href="/assets/js/44.8513f2c9.js"><link rel="prefetch" href="/assets/js/45.3a590740.js"><link rel="prefetch" href="/assets/js/46.4854a550.js"><link rel="prefetch" href="/assets/js/47.80212a21.js"><link rel="prefetch" href="/assets/js/48.1a83df73.js"><link rel="prefetch" href="/assets/js/49.aa599a69.js"><link rel="prefetch" href="/assets/js/5.bf342ddc.js"><link rel="prefetch" href="/assets/js/50.7b9ab880.js"><link rel="prefetch" href="/assets/js/51.8e16855c.js"><link rel="prefetch" href="/assets/js/52.8e6730bc.js"><link rel="prefetch" href="/assets/js/53.f2ab5058.js"><link rel="prefetch" href="/assets/js/54.cbc29320.js"><link rel="prefetch" href="/assets/js/55.c04f43e3.js"><link rel="prefetch" href="/assets/js/56.b097240f.js"><link rel="prefetch" href="/assets/js/57.f09ce6f9.js"><link rel="prefetch" href="/assets/js/58.0526911b.js"><link rel="prefetch" href="/assets/js/59.61d88944.js"><link rel="prefetch" href="/assets/js/6.9b7db570.js"><link rel="prefetch" href="/assets/js/60.1a1f0c25.js"><link rel="prefetch" href="/assets/js/61.328ea412.js"><link rel="prefetch" href="/assets/js/62.21cd989b.js"><link rel="prefetch" href="/assets/js/63.62a5ea56.js"><link rel="prefetch" href="/assets/js/64.e46e59c0.js"><link rel="prefetch" href="/assets/js/65.a9ceebdb.js"><link rel="prefetch" href="/assets/js/66.3de1336a.js"><link rel="prefetch" href="/assets/js/67.2da4f992.js"><link rel="prefetch" href="/assets/js/68.1e8182b6.js"><link rel="prefetch" href="/assets/js/69.37a87946.js"><link rel="prefetch" href="/assets/js/7.2571df95.js"><link rel="prefetch" href="/assets/js/70.659b256f.js"><link rel="prefetch" href="/assets/js/71.bb371526.js"><link rel="prefetch" href="/assets/js/72.b8db4a6c.js"><link rel="prefetch" href="/assets/js/73.3cd8b5b0.js"><link rel="prefetch" href="/assets/js/74.7f877c14.js"><link rel="prefetch" href="/assets/js/75.f7a92900.js"><link rel="prefetch" href="/assets/js/76.4de41963.js"><link rel="prefetch" href="/assets/js/77.ef073569.js"><link rel="prefetch" href="/assets/js/8.c64fe8ce.js"><link rel="prefetch" href="/assets/js/9.57cea2e1.js">
    <link rel="stylesheet" href="/assets/css/0.styles.fd010194.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">阿信 _notes</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/paper/" class="nav-link router-link-active">
  文献阅读
</a></div><div class="nav-item"><a href="/knowledge/" class="nav-link">
  零碎知识
</a></div><div class="nav-item"><a href="/work/" class="nav-link">
  日常工作
</a></div><div class="nav-item"><a href="/code/" class="nav-link">
  编程开发
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/paper/" class="nav-link router-link-active">
  文献阅读
</a></div><div class="nav-item"><a href="/knowledge/" class="nav-link">
  零碎知识
</a></div><div class="nav-item"><a href="/work/" class="nav-link">
  日常工作
</a></div><div class="nav-item"><a href="/code/" class="nav-link">
  编程开发
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>期刊文章</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/paper/期刊文章/01 基于UAV影像密集匹配点云多层次分割的建筑物层高变化检测.html" class="sidebar-link">基于UAV影像密集匹配点云多层次分割的建筑物层高变化检测</a></li><li><a href="/paper/期刊文章/02 基于基本形状及其拓扑关系的点云建筑物重建方法.html" class="sidebar-link">基于基本形状及其拓扑关系的点云建筑物重建方法</a></li><li><a href="/paper/期刊文章/03 利用基元分解的机载点云复杂建筑物自动重建.html" class="sidebar-link">利用基元分解的机载点云复杂建筑物自动重建</a></li><li><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html" class="sidebar-link">Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</a></li><li><a href="/paper/期刊文章/05 OpenDR An ApproximateDiﬀerentiable Renderer.html" class="sidebar-link">Open DR: An Approximate Diﬀerentiable  Renderer</a></li><li><a href="/paper/期刊文章/06 Neural 3D Mesh Renderer.html" class="sidebar-link">Neural 3D Mesh Renderer</a></li><li><a href="/paper/期刊文章/07 Soft Rasterizer A Differentiable Renderer for Image-based 3D Reasoning.html" class="sidebar-link">Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning</a></li><li><a href="/paper/期刊文章/09 Shape Reconstruction of Object-Level Building from Single Image Based on Implicit Representation Network.html" class="sidebar-link">Shape Reconstruction of Object-Level Building from Single Image Based on Implicit Representation Network</a></li><li><a href="/paper/期刊文章/10 Deep Learning for Image and Point Cloud Fusion in Autonomous Driving A Review.html" class="sidebar-link">Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A Review</a></li><li><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html" class="active sidebar-link">Differentiable Rendering: A Survey</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_1-引言" class="sidebar-link">1 引言</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-算法" class="sidebar-link">2 算法</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-1-网格" class="sidebar-link">2.1 网格</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-2-体素" class="sidebar-link">2.2 体素</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-3-点云" class="sidebar-link">2.3 点云</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-4-隐式表达" class="sidebar-link">2.4 隐式表达</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-5-神经渲染" class="sidebar-link">2.5 神经渲染</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-6-总结" class="sidebar-link">2.6 总结</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_2-7-公开问题" class="sidebar-link">2.7 公开问题</a></li></ul></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html#_3-评估" class="sidebar-link">3 评估</a></li></ul></li><li><a href="/paper/期刊文章/11 Outdoor inverse rendering from a single image using multiview self-supervision.html" class="sidebar-link">Outdoor inverse rendering from a single image using multiview self-supervision</a></li><li><a href="/paper/期刊文章/12 LGENet.html" class="sidebar-link">LGENet</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>学位论文</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>文献略读</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>报告讲座</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="differentiable-rendering-a-survey"><a href="#differentiable-rendering-a-survey" class="header-anchor">#</a> Differentiable Rendering: A Survey</h1> <p>2020</p> <p>**摘要：**深度神经网络（DNN）在目标检测或图像分割等视觉相关任务中表现出了显著的性能改进。尽管他们取得了成功，但他们通常缺乏对形成图像的3D对象的理解，因为不可能总是收集有关场景的3D信息或轻松地对其进行注释。可微绘制是一个新的领域，它允许计算三维对象的梯度并通过图像传播。它还降低了3D数据收集和注释的要求，同时在各种应用中实现了更高的成功率。本文回顾了现有的文献，讨论了可微绘制的现状、应用和有待解决的研究问题。</p> <p>**索引项：**可微绘制、逆图形、综合分析</p> <h2 id="_1-引言"><a href="#_1-引言" class="header-anchor">#</a> 1 引言</h2> <p>过去几年已经清楚地表明，神经网络对于二维和三维推理是有效的[1]、[2]、[3]、[4]、[5]、[6]。然而，大多数三维估计方法依赖于有监督的训练机制和昂贵的注释，这使得三维观测的所有属性的收集具有挑战性。因此，最近有人努力利用更容易获得的2D信息和不同级别的监督来理解3D场景。其中一种方法是将图形绘制过程集成到神经网络管道中。这允许将3D估计转换并合并到2D图像级证据中。</p> <p>计算机图形学中的渲染是生成由几何体、材质、场景灯光和相机属性定义的三维场景图像的过程。渲染是一个复杂的过程，它的差异不是唯一定义的，这妨碍了直接集成到神经网络中。</p> <p>微分渲染（DR）是一系列技术，通过获得渲染过程的有用梯度来解决端到端优化的集成问题。通过区分渲染，DR缩小了2D和3D处理方法之间的差距，允许神经网络在操作2D投影时优化3D实体。如图1所示，<strong>可以通过相对于渲染输出反向传播梯度来实现3D场景参数的优化</strong>。通过将渲染层与预测的场景参数进行集成，并通过以各种方式比较渲染图像和输入图像来应用损失，从而应用通用的3D自我监控管道。该过程的应用非常广泛，包括基于图像的三维物体重建训练[7]、[8]、[9]、人体姿势估计[10]、[11]、手姿势估计[12]、[13]和人脸重建[14]、[15]。</p> <img src="/assets/img/image-20220218101254088.f284c0aa.png" alt="image-20220218101254088" style="zoom:80%;"> <p>尽管有潜力，但使用现有或开发新的DR方法并不简单。这可归因于四个原因：</p> <ol><li>•在过去几年中，已经发表了许多基于DR的方法。为了了解哪些方法适合解决某些类型的问题，需要彻底了解这些方法的机制以及基本属性。</li> <li>•为了选择或开发新的DR方法，应了解现有方法的评估方法。</li> <li>•为了在新任务中使用DR，有必要调查现有应用中DR的使用情况。然而，由于应用的多样性，对该领域有一个清晰的认识并非易事。</li> <li>•在过去几年中，出现了几个DR库，每个库都专注于可微渲染过程的不同方面。这使得一些库对于特定类型的应用程序非常有用，而对于其他应用程序，可能需要从头开始实现额外的功能。此外，某些应用程序受到计算需求的限制，而基于DR的方法的现有实现通常无法满足这些需求。这对于实时应用或嵌入式设备尤其如此，对于这些应用或嵌入式设备，需要高度优化的神经网络。</li></ol> <p>**为了解决这些缺点，需要对DR的现状进行适当的调查。**然而，据我们所知，迄今为止还没有对此进行全面审查。在这项工作中，我们在第2节中概述了DR算法的当前状态，在第3节中概述了评估指标，在第4节中概述了使用可微渲染的应用程序，以及目前用于促进第5节研究的库。除了对现有方法进行调查外，我们还讨论了开放性研究问题，并对未来的工作提出了建议。</p> <h2 id="_2-算法"><a href="#_2-算法" class="header-anchor">#</a> 2 算法</h2> <img src="/assets/img/image-20220218102310103.d009bc8b.png" alt="image-20220218102310103" style="zoom:80%;"> <p>我们首先简要地定义一个数学公式。渲染函数R将形状参数Φs、相机参数Φc、材质参数Φm和照明参数Φl作为输入，并输出RGB图像Ic或深度图像Id。我们将输入表示为Φ={Φs，Φm，Φc，Φl}，输出表示为I={Ic，Id}。请注意，一般的DR公式可以有不同类型的额外输入/输出实体，但在本节中，我们将介绍最常见的输入/输出实体。可微渲染器计算输出图像相对于输入参数的梯度∂I/∂Φ，以优化特定目标（损失函数）。这些梯度的计算可以是近似的，但应该足够精确，以传播最小化目标函数所需的有意义信息。</p> <p>渲染函数Φ的输入，尤其是几何参数Φs，是可微渲染算法之间的一些主要区别。每种数据表示都有自己的优点，因此适合解决特定问题。在这项工作中，我们根据底层数据表示将调查的DR算法分为四类：网格、体素、点云和神经隐式表示。我们还讨论了神经渲染，因为越来越多的研究是学习使用神经网络模型进行渲染，而不是手动设计渲染及其差异化。<strong>表1</strong>列出了我们在本节中涉及的文献。</p> <h3 id="_2-1-网格"><a href="#_2-1-网格" class="header-anchor">#</a> 2.1 网格</h3> <h4 id="_2-1-1-渲染的分析导数"><a href="#_2-1-1-渲染的分析导数" class="header-anchor">#</a> 2.1.1 渲染的分析导数</h4> <p>网格将三维形状表示为一组顶点和连接它们的曲面。它被广泛应用，尤其是在计算机图形学中，因为它可以以紧凑的方式表示复杂的三维形状。</p> <p>给定渲染输入和像素，确定其颜色的过程可分为（1）为像素指定三角形和（2）基于指定三角形顶点的颜色计算像素的颜色。前者是通过将所有网格三角形从世界空间投影到屏幕空间，并确定包围像素的三角形来实现的。然后选择离相机最近的三角形。由于选择会产生一个离散的三角形标识符，因此该操作对于所有参数都是不可微的。</p> <p>所有进一步的运算都是可微的。图2显示了简化的计算流程。首先，三角形、灯光的位置和方向从世界空间投影到相机空间（图2（c）、（d）），然后转换到屏幕空间（图2（b））。这些运算是可微的，因为它们是通过简单的矩阵积完成的。然后，像素坐标可以表示为三个顶点的加权和。权重（图2（a））的计算是通过求解（可微）线性程序来完成的，而像素深度是通过插值相机空间中三个顶点的深度来计算的（图2（d））。类似地，像素处的材质和法向量通常表示为三角形顶点处定义的材质和法向量的加权和。对于局部照明模型，给定材质和照明参数以及像素处的法向量，可以使用反射模型计算像素颜色（图2（e））。常见的反射模型，如Phong[47]、Lambertian[48]和球面谐波[49]都是可微的。因此，可以解析地计算像素深度和颜色相对于输入参数的导数[16]、[17]、[18]。</p> <p><img src="/assets/img/image-20220218115034503.34796a86.png" alt="image-20220218115034503"></p> <p>在标准渲染中，通常每像素只选择一个三角形来计算最终颜色值，这可能会导致优化问题。为了举例说明，让我们考虑一个由一个白色三角形和两个黑色三角形组成的场景，如图3所示。vwi的顶点颜色为1，vbi的顶点颜色为0。然后，使用满足vp=wivwi和wi=1的重心坐标wi，vp处像素的颜色是恒定的cvp=wicvwi=1。因此，cvp相对于vwi和vbi的梯度为零。</p> <p><em><strong>（leung）意思是：常规渲染中，Vp处的颜色是白色三角形3个顶点加权算出的结果，始终是1（1代表白色）</strong></em></p> <p>类似地，对于所有像素和顶点，像素颜色相对于顶点位置的导数始终为零。因此，在这种情况下，解析导数无助于优化几何体。然而，在实践中，顶点的位置会影响像素颜色。例如，当vw2向右移动时，cvp将变为0。因此，我们可以通过允许无关像素的颜色影响相邻三角形来解决这个问题。有几种渲染方法提供了反映这些见解的近似梯度[19]、[9]、[20]、[14]，而其他方法则通过近似光栅化器过程[22]、[23]、[21]来克服这个问题。</p> <h4 id="_2-1-2-近似梯度"><a href="#_2-1-2-近似梯度" class="header-anchor">#</a> 2.1.2 近似梯度</h4> <p>Loper和Black[19]在第一个通用可微渲染器OpenDR中使用了近似的空间梯度。vp可以用wi=1的vp=wivwi表示，并且可以使用差分滤波器（例如Sobel滤波器）计算像素相对于vp的导数。换句话说{∂cvp/∂x，∂cvp/∂y}=∂cv/p∂副总裁。由于计算梯度时会考虑位于vp左右的像素，因此在该公式中，它可能具有非零值。</p> <p>Kato等人[9]提出了OpenDR的两个问题，并提出了一个名为neural 3D mesh renderer（NMR）的渲染器。第一个问题是梯度计算的局部性。由于OpenDR中差分过滤器的局部性，只有边界像素上的梯度可以流向顶点，而其他像素上的梯度不能使用。基于此特性的优化可能会导致局部极小值很低。第二个问题是导数没有利用目标应用的损失梯度，例如图像重建。例如，在图3的情况下，如果目标是降低vp的强度，我们应该替换vw2。然而，如果目标是增加，我们就不应该这样做。因此，梯度应该是客观的，以便更好地优化。由于OpenDR的目标不是提供精确的梯度，而是为优化提供有用的梯度，因此需要一个损失感知梯度流。为了克服这些问题，作者提出了非局部近似梯度，也使用了从损失函数反向传播的像素梯度。作者后来用类似于OpenDR的局部梯度替换了非局部梯度，以降低计算复杂度[20]。</p> <p>Genova等人[14]使用每个三角形相对于每个像素的重心坐标计算光栅化导数。它们为位于三角形边界外的像素的重心坐标引入负值，以克服遮挡不连续性。通过省略三角形标识符和使用负重心坐标，形状可以被视为局部平面，以近似遮挡边界。然而，在优化平移或遮挡时，这种近似可能会带来问题。</p> <h4 id="_2-1-3-近似渲染"><a href="#_2-1-3-近似渲染" class="header-anchor">#</a> 2.1.3 近似渲染</h4> <p>为了能够计算出有用的渐变，其他方法不是近似向后过程，而是近似渲染的光栅化（或向前过程）。</p> <p>罗丁等人[21]重新解释了场景参数，以确保可区分性。为了防止硬对象边界的不连续性，每个对象都由一个密度参数定义，该参数在对象中心具有最大不透明性，并且对边界透明。因此，渲染结果边缘模糊且平滑，而从场景参数中移除锐角可确保可区分性。</p> <p>Liu等人[22]采用类似的方法，提出了一种名为软光栅化器的渲染器。除了空间模糊环之外，它还用概率方法取代了普通光栅化过程中基于z缓冲区的三角形选择，在这种方法中，投影到像素pi上的每个三角形都以一定的概率贡献其颜色。实际上，聚合函数融合了每个像素的所有颜色概率。结果，每个像素颜色都被计算为对应于相关三角形的值的加权和，并且该操作是可微的。权重基于二维屏幕空间中像素和三角形之间的距离，以及相机和三角形之间沿观察方向的距离。因此，梯度以概率方式在整个图像上累积信息，而OpenDR仅从相邻像素反向传播到顶点，而NMR仅反向传播[min（objx）、max（objx）]和[min（objy）、max（objy）]范围内像素的梯度。请注意，上一节中的所有方法都不提供对向前传球的控制，因为它们的目标只是近似向后坡度。</p> <p>Chen等人[23]提出了DIB-R，它独立关注图像的两个不同区域：前景像素（其中一个像素被至少一个人脸覆盖）和背景像素（没有任何人脸覆盖）。为了避免软光栅化器的模糊输出，DIB-R建议对前景像素使用分析导数，使用人脸顶点在属性处的重心插值计算。它还通过使用基于距离的全局人脸信息聚合，以类似于软光栅化的方式，防止背景像素的梯度消失。</p> <h4 id="_2-1-4-全局照明"><a href="#_2-1-4-全局照明" class="header-anchor">#</a> 2.1.4 全局照明</h4> <p>图2中的管道不适用于全局照明模型，因为像素的照明不受其他曲面点反射光的影响。尽管这种简化缩短了渲染时间，但生成包含光、几何体和材质复杂交互的真实照片图像变得不可能。在全局照明模型中，使用渲染方程[50]的蒙特卡罗估计来计算像素的颜色。	可微真实感绘制中的主要挑战是，当积分包含因对象轮廓而产生的不连续性时，估计蒙特卡罗估计绘制方程的积分对应的导数。</p> <p>Li等人[24]是第一个计算基于物理的渲染图像上的标量函数相对于相机、光材料和几何体等任意输入参数的导数的工作。它使用一种基于蒙特卡罗射线追踪的随机方法，估计像素滤波器积分的积分和梯度。由于边缘和遮挡本质上是不连续的，因此积分计算被分为平滑区域和不连续区域。对于整数和的平滑部分，采用了具有自动微分的传统区域采样。对于不连续零件，引入了一种新的边缘采样方法来捕捉边界处的变化。他们的方法做出了某些假设：网格没有穿透，没有点光源，没有完美的镜面反射，场景是静态的。Zhang等人[25]提出了一种非常类似的方法。与Li等人[24]不同的是，他们的方法支持除三角形网格外的体积差异。这些方法的两个主要缺点是渲染速度和估计梯度的较大差异。这是因为找到所有对象边缘并对其进行采样是一项具有挑战性的任务，这需要许多样本。</p> <p>..</p> <p>..</p> <p>..</p> <h3 id="_2-2-体素"><a href="#_2-2-体素" class="header-anchor">#</a> 2.2 体素</h3> <p>在本节中，我们将介绍使用体素表示数据的可微分渲染算法。体素是三维空间的单位立方体表示。它可以被参数化为一个N维向量，其中包含有关三维空间中所占体积的信息，以及其他信息。通常情况下，使用二进制值或非二进制值对体素中的占用信息进行编码。对于预定占用率的应用，非二进制占用概率∈ [pmin，pmax]通常被存储。尽管占用概率与透明度值不同，但它们可以用相同的方式进行解释，以便在光线行进操作期间保持可微性。在这种情况下，概率决定了光线在某一点的吸收（透明度）。材料信息也经常被存储。形状可以表示为从每个体素中心到对象表面的最短距离，而不是存储占用率。在这种表示法中，每个体素单元都指定了一个距离函数（DF）。距离函数可以增加一个符号值，表示体素是包含在对象内部还是外部，以形成符号距离函数（SDF）。或者，可以将截断应用于SDF，以形成截断有符号距离函数（TSDF），用于仅对对象曲面的距离信息很重要的应用。</p> <h3 id="_2-3-点云"><a href="#_2-3-点云" class="header-anchor">#</a> 2.3 点云</h3> <p>点云是表示三维空间中形状的点的集合。它们在3D Vision社区中无处不在，因为它们能够以相对较低的存储成本表示多种拓扑。此外，目前市场上的大多数3D传感器也依赖于点云来编码数据。近年来的研究表明，点云可以成功地集成到深层神经网络中，以解决各种实际的3D问题[6]、[53]、[5]。鉴于微分渲染的出现及其在减少3D监控的场景理解方面的潜力，点云因此成为数据表示的自然选择。</p> <h3 id="_2-4-隐式表达"><a href="#_2-4-隐式表达" class="header-anchor">#</a> 2.4 隐式表达</h3> <p>最近，人们对在神经网络中以参数化方式表示几何信息越来越感兴趣[54]、[55]、[56]。这通常被称为神经内隐表征。在该模型中，点Pixyz∈ r3处的几何信息由神经网络F（Pixyz）的输出来描述。与基于体素的方法不同，在隐式表示中，内存的使用与空间分辨率有关。因此，曲面可以无限分辨率重建，而不会占用过多内存。</p> <h3 id="_2-5-神经渲染"><a href="#_2-5-神经渲染" class="header-anchor">#</a> 2.5 神经渲染</h3> <p>Eslamiet al.[58]建议从数据中学习渲染过程，而不是手工制作渲染差异。这种方法通常被称为神经渲染。通常，通过最小化图像重建误差来联合训练输出神经场景表示的场景生成网络和渲染网络。由于神经网络的最新发展，神经渲染现在能够生成高质量的图像，并被用于许多应用，例如新颖的视图合成、语义照片处理、面部和身体再现、重新照明、自由视点视频以及照片逼真的化身的创建。</p> <p>虽然手工绘制的渲染器并不总是准确地模拟物理世界，但通过从真实世界数据中学习，神经渲染可以生成与真实世界无法区分的新图像。另一方面，推广到与训练数据不同的场景、扩展到由多个对象组成的场景以及人类修改的能力都是有限的。改进神经渲染的一个有希望的方向是为3D场景添加诱导偏差，并将其渲染到神经网络中[59]、[31]、[60]。因此，将基于归纳偏差的可微渲染器与神经渲染相结合将是一个有趣的研究领域。</p> <p>有关神经渲染的更多细节，请参考最近的一项调查[61]。</p> <h3 id="_2-6-总结"><a href="#_2-6-总结" class="header-anchor">#</a> 2.6 总结</h3> <p>我们提出了基于四种数据表示类型的可微绘制技术：网格、体素、点云和隐式函数。图4说明了这些表示。本节的要点总结如下：</p> <p><img src="/assets/img/image-20220218174657486.11c2540d.png" alt="image-20220218174657486"></p> <p><em>图4：不同的渲染算法在如何收集和聚合沿光线的几何信息方面有所不同。对于体素，通过检查光线和每个体素的交点来收集几何信息。对于网格，与不可微渲染不同，多个多边形必须与一条光线关联。对于点云，有几种方法可以通过伪大小测量点对光线的影响。对于神经隐函数，为了提高效率，人们提出了各种采样技术。聚合方法主要取决于几何信息是确定性处理还是概率处理，以及如何处理遮挡。</em></p> <p><strong>基于网格的方法可以分为三类</strong>：近似梯度、近似渲染和全局照明。梯度近似允许高效和高质量的光栅化，同时旨在手工制作有意义的梯度。渲染近似可能会产生模糊的输出，但会确保所有像素的梯度均为非零。基于全局照明的技术减少了渲染图像和真实世界数据之间的域间距，但由于计算成本高，目前无法用于深度神经网络。</p> <p><strong>基于体素的方法</strong>易于使用，但需要过多的内存和参数使用。沿光线收集和聚集体素，以产生最终的像素值，是基于体素的方法的一个主要区别因素。尽管这些方法简单易行，但其适用性仅限于小场景和低分辨率。基于SDF的体素方法可以比基于栅格的方法更平滑地表示曲面。</p> <p><strong>基于点云的方法</strong>提供了较低的计算成本，同时面临大小不确定性。点云是许多可差分渲染方法的自然选择，因为它们简单且广泛用于3D传感器，但无法捕获密集的曲面信息。选择一个合适的点大小并在遮挡情况下决定渲染像素的颜色并不简单。人们提出了不同的方法来解决这些问题。</p> <p><strong>隐式表示</strong>可能是点云、体素和网格的可行替代方案，但在沿光线采样点时，计算成本很高。隐式函数通过占用概率、透明度或到曲面的距离来描述三维点处的几何体。广泛的拓扑结构可以以几乎无限的分辨率和较低的内存成本来表示。尽管有这些优点，但在射线上聚集数据可能需要大量计算，因为对它们进行采样需要在大量点上对神经网络进行评估。</p> <p><img src="/assets/img/image-20220218180951932.fc09054e.png" alt="image-20220218180951932"></p> <p>表3总结了不同表示和渲染方法的强度和限制。</p> <h3 id="_2-7-公开问题"><a href="#_2-7-公开问题" class="header-anchor">#</a> 2.7 公开问题</h3> <p>目前的可微绘制方法没有解决几个问题。除了3D对抗示例和样式转换之外，3D形状估计和姿势估计等许多应用程序都通过最小化渲染图像和目标图像之间的差异来训练神经网络。在这种管道中，渲染函数和图像比较函数通常是分开开发的。因此，比较函数无法利用提供给渲染函数的丰富3D信息。然而，他们也可以一起考虑。如第2.1节所述，可微分渲染的目的不是提供精确的梯度，而是提供对优化有用的梯度。这可以通过采用3D模型和目标2D图像的可微分渲染和比较功能来实现，而不是采用3D模型并输出图像的可微分渲染功能。这个方向上的一种方法是对与3D模板形状顶点相关的关键点进行可微投影和比较[62]。尽管这种方法是特定于任务的，但可以通过在渲染和图像比较函数中引入3D信息来推广。这些信息的整合仍然是一个开放的研究领域。</p> <p>当前基于局部照明的可微分渲染方法非常简单，无法生成显示阴影和反射的照片级真实感图像。此外，全局照明方法对于训练神经网络来说太慢。另一方面，在游戏引擎中，实时渲染方法的进步使得在不使用计算密集型路径跟踪的情况下渲染高度逼真的图像成为可能。然而，在可差分渲染的背景下，这两种渲染之间的区域尚未探索。集成快速但复杂的实时渲染方法（如阴影贴图和环境贴图）是一种尚未尝试过的方法，但有可能提高渲染质量。</p> <p>视频的可微渲染也是一个值得探索的有趣研究方向。为了实现这一点，需要将可微渲染与物理模拟器集成（以纳入额外的物理约束）。从理论上讲，应该可以训练一条端到端的管道，将视频数据与可微物理模拟器结合起来[63]，[64]，但这还没有经过实验。</p> <p>由于渲染假设使用物理模型生成图像，因此如果物理模型与真实世界之间存在较大差异，渲染图像将显得不现实。另一方面，神经渲染可以生成高度逼真的图像，因为它几乎不需要对物理模型进行任何假设。然而，这样的假设有时可能会产生违反物理模型的图像。例如，在神经渲染中，对象的形状可能因视点而异。由神经网络渲染的对象[60]和场景[59]的图像不能保证不同视点的形状一致性。虽然将物理世界的归纳偏见纳入神经渲染的方法很重要，但将基于学习的方法纳入可微渲染也值得考虑。人类可以直观地、即时地理解相机移动时场景将如何变化。这种能力基于过去的经验，而不是计算大脑中每个像素的值。因此，这种学习可能有助于梯度计算。基于学习的方法已经用于渲染中的图像去噪[65]、[66]和高效光线采样[67]。这些方法在可微绘制中可能也很有用。</p> <h2 id="_3-评估"><a href="#_3-评估" class="header-anchor">#</a> 3 评估</h2> <p>由于渲染是一个复杂的函数，可微渲染方法的评估不是一个简单的问题。在本节中，我们将回顾评估算法的常见做法，并提出它们的问题。</p> <p>一种简单的方法是直接梯度评估。对于基于全局照明的方法[24]、[25]、[26]、[27]，当渲染积分包含可见性项（如对象边界）时，一种有效的梯度计算方法仍然是一个有待解决的研究问题。由于目标是计算分析正确的梯度，因此使用有限差分计算的梯度作为基础真值。然而，由于缺乏通用的评估数据集，无法对算法进行定量评估。另一方面，一些文献[19]、[9]、[20]侧重于计算局部照明模型的近似梯度。在这种情况下，梯度应该对优化有意义，而不是在分析上正确。由于这个原因，有限差分不能用于基本事实。它们仅近似于分析正确的导数，因此可能不适用于优化。Loper和Black[19]将他们的方法计算的梯度与有限差分计算的梯度进行了比较，并声称提出的方法更好。他们的理由是，在有限差分法中确定一个好的epsilon进行优化是一个挑战。在优化目标函数的过程中，可视化梯度（不显示地面真相）并分析其收敛效率是另一种用于评估的方法[9]，[24]。</p> <p>评估优化的场景参数是另一种正在使用的方法[19]、[22]、[23]。由于缺乏用于比较的通用数据集，每篇论文都使用自己的数据集来评估算法。许多论文没有直接优化三维场景的参数，而是训练神经网络进行单视图三维对象重建，并报告重建精度[9]、[20]、[7]、[8]、[22]、[23]。</p> <p>计算时间也是一个重要指标，尤其是对于基于光线跟踪的渲染方法。因此，有几篇论文将计算时间作为评估方法的一部分[19]、[24]、[26]。</p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/paper/期刊文章/10 Deep Learning for Image and Point Cloud Fusion in Autonomous Driving A Review.html" class="prev">
        Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A Review
      </a></span> <span class="next"><a href="/paper/期刊文章/11 Outdoor inverse rendering from a single image using multiview self-supervision.html">
        Outdoor inverse rendering from a single image using multiview self-supervision
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.4da1065c.js" defer></script><script src="/assets/js/3.d419b5b7.js" defer></script><script src="/assets/js/22.7ea6fe99.js" defer></script>
  </body>
</html>
