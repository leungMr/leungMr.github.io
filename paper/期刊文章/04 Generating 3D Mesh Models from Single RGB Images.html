<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images | 阿信 _notes</title>
    <meta name="generator" content="VuePress 1.9.7">
    
    <meta name="description" content="">
    
    <link rel="preload" href="/assets/css/0.styles.fd010194.css" as="style"><link rel="preload" href="/assets/js/app.4da1065c.js" as="script"><link rel="preload" href="/assets/js/3.d419b5b7.js" as="script"><link rel="preload" href="/assets/js/21.ffea2103.js" as="script"><link rel="prefetch" href="/assets/js/10.4bad6486.js"><link rel="prefetch" href="/assets/js/11.3dcdeb34.js"><link rel="prefetch" href="/assets/js/12.4bed0e42.js"><link rel="prefetch" href="/assets/js/13.b6a43f3c.js"><link rel="prefetch" href="/assets/js/14.3f3df886.js"><link rel="prefetch" href="/assets/js/15.1266ea8b.js"><link rel="prefetch" href="/assets/js/16.bffd9bb8.js"><link rel="prefetch" href="/assets/js/17.97b56ab2.js"><link rel="prefetch" href="/assets/js/18.44f604f8.js"><link rel="prefetch" href="/assets/js/19.a02c01a8.js"><link rel="prefetch" href="/assets/js/2.5d29b4a8.js"><link rel="prefetch" href="/assets/js/20.022f3328.js"><link rel="prefetch" href="/assets/js/22.7ea6fe99.js"><link rel="prefetch" href="/assets/js/23.acf69f2c.js"><link rel="prefetch" href="/assets/js/24.6f720be1.js"><link rel="prefetch" href="/assets/js/25.e5f77a77.js"><link rel="prefetch" href="/assets/js/26.2789ae5a.js"><link rel="prefetch" href="/assets/js/27.2e549eb0.js"><link rel="prefetch" href="/assets/js/28.e17e02be.js"><link rel="prefetch" href="/assets/js/29.67e72e71.js"><link rel="prefetch" href="/assets/js/30.e3dda54c.js"><link rel="prefetch" href="/assets/js/31.210574c3.js"><link rel="prefetch" href="/assets/js/32.6ba63bb2.js"><link rel="prefetch" href="/assets/js/33.45d07bfa.js"><link rel="prefetch" href="/assets/js/34.3281ef49.js"><link rel="prefetch" href="/assets/js/35.b634cff1.js"><link rel="prefetch" href="/assets/js/36.491b73c3.js"><link rel="prefetch" href="/assets/js/37.e35dba7c.js"><link rel="prefetch" href="/assets/js/38.0756e011.js"><link rel="prefetch" href="/assets/js/39.675f9f54.js"><link rel="prefetch" href="/assets/js/4.a4009e16.js"><link rel="prefetch" href="/assets/js/40.8fe858a4.js"><link rel="prefetch" href="/assets/js/41.81a5670f.js"><link rel="prefetch" href="/assets/js/42.83bf3204.js"><link rel="prefetch" href="/assets/js/43.c724dca3.js"><link rel="prefetch" href="/assets/js/44.8513f2c9.js"><link rel="prefetch" href="/assets/js/45.3a590740.js"><link rel="prefetch" href="/assets/js/46.4854a550.js"><link rel="prefetch" href="/assets/js/47.80212a21.js"><link rel="prefetch" href="/assets/js/48.1a83df73.js"><link rel="prefetch" href="/assets/js/49.aa599a69.js"><link rel="prefetch" href="/assets/js/5.bf342ddc.js"><link rel="prefetch" href="/assets/js/50.7b9ab880.js"><link rel="prefetch" href="/assets/js/51.8e16855c.js"><link rel="prefetch" href="/assets/js/52.8e6730bc.js"><link rel="prefetch" href="/assets/js/53.f2ab5058.js"><link rel="prefetch" href="/assets/js/54.cbc29320.js"><link rel="prefetch" href="/assets/js/55.c04f43e3.js"><link rel="prefetch" href="/assets/js/56.b097240f.js"><link rel="prefetch" href="/assets/js/57.f09ce6f9.js"><link rel="prefetch" href="/assets/js/58.0526911b.js"><link rel="prefetch" href="/assets/js/59.61d88944.js"><link rel="prefetch" href="/assets/js/6.9b7db570.js"><link rel="prefetch" href="/assets/js/60.1a1f0c25.js"><link rel="prefetch" href="/assets/js/61.328ea412.js"><link rel="prefetch" href="/assets/js/62.21cd989b.js"><link rel="prefetch" href="/assets/js/63.62a5ea56.js"><link rel="prefetch" href="/assets/js/64.e46e59c0.js"><link rel="prefetch" href="/assets/js/65.a9ceebdb.js"><link rel="prefetch" href="/assets/js/66.3de1336a.js"><link rel="prefetch" href="/assets/js/67.2da4f992.js"><link rel="prefetch" href="/assets/js/68.1e8182b6.js"><link rel="prefetch" href="/assets/js/69.37a87946.js"><link rel="prefetch" href="/assets/js/7.2571df95.js"><link rel="prefetch" href="/assets/js/70.659b256f.js"><link rel="prefetch" href="/assets/js/71.bb371526.js"><link rel="prefetch" href="/assets/js/72.b8db4a6c.js"><link rel="prefetch" href="/assets/js/73.3cd8b5b0.js"><link rel="prefetch" href="/assets/js/74.7f877c14.js"><link rel="prefetch" href="/assets/js/75.f7a92900.js"><link rel="prefetch" href="/assets/js/76.4de41963.js"><link rel="prefetch" href="/assets/js/77.ef073569.js"><link rel="prefetch" href="/assets/js/8.c64fe8ce.js"><link rel="prefetch" href="/assets/js/9.57cea2e1.js">
    <link rel="stylesheet" href="/assets/css/0.styles.fd010194.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">阿信 _notes</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/paper/" class="nav-link router-link-active">
  文献阅读
</a></div><div class="nav-item"><a href="/knowledge/" class="nav-link">
  零碎知识
</a></div><div class="nav-item"><a href="/work/" class="nav-link">
  日常工作
</a></div><div class="nav-item"><a href="/code/" class="nav-link">
  编程开发
</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/paper/" class="nav-link router-link-active">
  文献阅读
</a></div><div class="nav-item"><a href="/knowledge/" class="nav-link">
  零碎知识
</a></div><div class="nav-item"><a href="/work/" class="nav-link">
  日常工作
</a></div><div class="nav-item"><a href="/code/" class="nav-link">
  编程开发
</a></div> <!----></nav>  <ul class="sidebar-links"><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading open"><span>期刊文章</span> <span class="arrow down"></span></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/paper/期刊文章/01 基于UAV影像密集匹配点云多层次分割的建筑物层高变化检测.html" class="sidebar-link">基于UAV影像密集匹配点云多层次分割的建筑物层高变化检测</a></li><li><a href="/paper/期刊文章/02 基于基本形状及其拓扑关系的点云建筑物重建方法.html" class="sidebar-link">基于基本形状及其拓扑关系的点云建筑物重建方法</a></li><li><a href="/paper/期刊文章/03 利用基元分解的机载点云复杂建筑物自动重建.html" class="sidebar-link">利用基元分解的机载点云复杂建筑物自动重建</a></li><li><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html" class="active sidebar-link">Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html#_3-本文模型方法" class="sidebar-link">3 本文模型方法</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html#_3-1-主要模型方法" class="sidebar-link">3.1 主要模型方法</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html#_3-2-各个关键部分" class="sidebar-link">3.2 各个关键部分</a></li></ul></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html#_4-实验" class="sidebar-link">4 实验</a><ul class="sidebar-sub-headers"><li class="sidebar-sub-header"><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html#_4-1-实验设置" class="sidebar-link">4.1 实验设置</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html#_4-2-与最新技术的对比" class="sidebar-link">4.2 与最新技术的对比</a></li><li class="sidebar-sub-header"><a href="/paper/期刊文章/04 Generating 3D Mesh Models from Single RGB Images.html#_4-3-消融实验" class="sidebar-link">4.3 消融实验</a></li></ul></li></ul></li><li><a href="/paper/期刊文章/05 OpenDR An ApproximateDiﬀerentiable Renderer.html" class="sidebar-link">Open DR: An Approximate Diﬀerentiable  Renderer</a></li><li><a href="/paper/期刊文章/06 Neural 3D Mesh Renderer.html" class="sidebar-link">Neural 3D Mesh Renderer</a></li><li><a href="/paper/期刊文章/07 Soft Rasterizer A Differentiable Renderer for Image-based 3D Reasoning.html" class="sidebar-link">Soft Rasterizer: A Differentiable Renderer for Image-based 3D Reasoning</a></li><li><a href="/paper/期刊文章/09 Shape Reconstruction of Object-Level Building from Single Image Based on Implicit Representation Network.html" class="sidebar-link">Shape Reconstruction of Object-Level Building from Single Image Based on Implicit Representation Network</a></li><li><a href="/paper/期刊文章/10 Deep Learning for Image and Point Cloud Fusion in Autonomous Driving A Review.html" class="sidebar-link">Deep Learning for Image and Point Cloud Fusion in Autonomous Driving: A Review</a></li><li><a href="/paper/期刊文章/10 Differentiable Rendering-A Survey.html" class="sidebar-link">Differentiable Rendering: A Survey</a></li><li><a href="/paper/期刊文章/11 Outdoor inverse rendering from a single image using multiview self-supervision.html" class="sidebar-link">Outdoor inverse rendering from a single image using multiview self-supervision</a></li><li><a href="/paper/期刊文章/12 LGENet.html" class="sidebar-link">LGENet</a></li></ul></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>学位论文</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>文献略读</span> <span class="arrow right"></span></p> <!----></section></li><li><section class="sidebar-group collapsable depth-0"><p class="sidebar-heading"><span>报告讲座</span> <span class="arrow right"></span></p> <!----></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="pixel2mesh-generating-3d-mesh-models-from-single-rgb-images"><a href="#pixel2mesh-generating-3d-mesh-models-from-single-rgb-images" class="header-anchor">#</a> Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images</h1> <p>ECCV 2018，代码开放</p> <p>Github：<a href="https://github.com/nywang16/Pixel2Mesh" target="_blank" rel="noopener noreferrer">nywang16/Pixel2Mesh: Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images. In ECCV2018. (github.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></p> <p>笔记参考：</p> <blockquote><ol><li><a href="https://www.jianshu.com/p/6c4bac018f79" target="_blank" rel="noopener noreferrer">论文 | Pixel2Mesh三维重建模型解读《Pixel2Mesh: Generating 3D Mesh Models from Single RGB Images》 - 简书 (jianshu.com)<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ol></blockquote> <h2 id="_3-本文模型方法"><a href="#_3-本文模型方法" class="header-anchor">#</a> 3 本文模型方法</h2> <h3 id="_3-1-主要模型方法"><a href="#_3-1-主要模型方法" class="header-anchor">#</a> 3.1 主要模型方法</h3> <p><img src="/assets/img/image-20211213172922809.b42a034c.png" alt="image-20211213172922809"></p> <ol><li>给定一张输入图像：Input image</li> <li>为任意的输入图像都初始化一个椭球体作为其初始三维形状：Ellipsoid Mesh</li> <li>整个网络可以大概分成上下两个部分；
<ul><li>上面部分负责用全卷积神经网络提取输入图像的特征；</li> <li>下面部分负责用图卷积神经网络来表示三维mesh，并对三维mesh不断进行形变，目标是得到最终的输出（最后边的飞机）</li></ul></li> <li>注意到图中的<code>perceptual feature pooling</code>层将上面的2D图像信息和下面的3Dmesh信息联系在一起了，即通过借鉴2D图像特征来调整3D mesh中的图卷积网络的节点状态。这个过程可以看成是<code>mesh deformation</code>.</li> <li>细心的同学应该也注意到除了刚刚提到的<code>mesh deformation</code>，下面这部分还有一个很关键的组成是<code>graph uppooling</code>。文章提出这个图上采样层是为了让图节点依次增加，从图中可以直接看到节点数是由156--&gt;628--&gt;2466变换的，这其实就是coarse-to-fine的体现。</li></ol> <h3 id="_3-2-各个关键部分"><a href="#_3-2-各个关键部分" class="header-anchor">#</a> 3.2 各个关键部分</h3> <h4 id="_3-2-1-图卷积神经网络gcn"><a href="#_3-2-1-图卷积神经网络gcn" class="header-anchor">#</a> 3.2.1 图卷积神经网络GCN</h4> <p>图卷积神经网络的定义如下：</p> <img src="/assets/img/image-20211213151022158.33373307.png" alt="image-20211213151022158" style="zoom:67%;"> <p>其中：</p> <ul><li>f(p,l), f(p,l+1)分别表示顶点p在卷积操作前后的特征向量；</li> <li>N(p)指顶点p的邻居节点；</li> <li>W1,W2表示待学习的参数；</li></ul> <p>其实整个公式就是表达了图卷积神经网络的<strong>节点</strong>是根据其自身的特征和邻居节点的特征来进行更新的。</p> <p>这样就解决了3D mesh的表示问题以及如何更新节点状态的问题。</p> <p>3.2.2 融合2D和3D信息</p> <p>perceptual feature pooling &amp; mesh deformation block</p> <p>文中用了经典的VGG网络来提取二维的图像信息，而用GCN来表示3D mesh，那么如何在两个不同的模态的数据之间融合工作，更好的利用2D的图像信息来帮助重建3D mesh</p> <p><img src="/assets/img/image-20211213172845338.737dc6a1.png" alt="image-20211213172845338"></p> <p><strong>mesh deformation block</strong></p> <ol><li><p>如上图(a)所示。</p></li> <li><p>C表示三维顶点坐标，P表示图像特征，F表示三维顶点特征；</p></li> <li><p><code>perceptual feature pooling</code>层负责根据三维顶点坐标C(i-1)去图像特征P中提取对应的信息；</p></li> <li><p>以上提取到的各个顶点特征再与上一时刻的顶点特征F(i-1)做融合，作为G-ResNet的输入；</p></li> <li><p>G-ResNet(graph-based ResNet)产生的输出又做为mesh deformable block的输出，得到新的三维坐标C(i)和三维顶点特征F(i)。</p></li></ol> <p><strong>perceptural feature pooling</strong></p> <ol><li>如上图(b)所示。</li> <li>负责给定三维坐标点以及图像特征的情况下，获取到三维点对应的特征信息。</li> <li>首先将3D坐标信息映射回2D坐标点；</li> <li>取2D坐标点边上最近的四个点进行双线性插值，其结果做为这个顶点的特征；</li> <li>特别的文章中取了VGG中的<code>conv3_3</code>（256维）,<code>conv4_3</code>（512维）,<code>conv_5_3</code>（512维）的特征进行连接，那么每个顶点就有1280维的特征。</li> <li>除了最开始的block没有F(i-1)的信息外，其他的block都还能利用上一时刻的128维度的F信息，一共1408维。</li></ol> <p><strong>G-ResNet</strong></p> <ol><li>如上图(a)中间的部分。</li> <li>前面为每个顶点都得到了1408维的特征（除了第一个block）通过G-ResNet就能得到新的位置坐标C和每个顶点的形状特征F；</li> <li>这就需要节点之间有效的信息交换，但每次图卷积网络只能交换邻居节点的信息，很影响新的交换效率，有点类似2D CNN的小感受野。所以增加了shortcut结构。</li> <li>每个block的G-ResNet的结构都是一样的（14个conv + 1 shortcut），输出128维，这样就产生新的128维的节点形状信息。</li></ol> <h4 id="_3-3-3-图上采样-graph-upooling"><a href="#_3-3-3-图上采样-graph-upooling" class="header-anchor">#</a> 3.3.3 图上采样（Graph upooling）</h4> <ol><li>主要是为了节点数量能够逐渐增加，降低训练难度；</li> <li>主要有face-based和edge-based这2种方式，都比较好理解，就不再解释了；</li> <li>文章中采用的edge-based的方式。</li></ol> <p><img src="/assets/img/image-20211213172905426.a63ba7d4.png" alt="image-20211213172905426"></p> <h4 id="_3-3-4-损失函数-losses"><a href="#_3-3-4-损失函数-losses" class="header-anchor">#</a> 3.3.4 损失函数 losses</h4> <p>文章一共为网络模型设计了4种不同的loss，来从不同角度保证网络模型的性能。</p> <ol><li>**Chamfer 损失函数：**其作用是限制网格顶点的具体位置；（倒角）</li> <li>**Normal 损失函数：**其作用是增强网格表面法向的一致性，增加表面光滑度；（表面法向损失）</li> <li>**Laplacian 正则化：**其作用是在形变时维持临近顶点的相对位置；（拉普拉斯损失）</li> <li>**Edge length 正则化：**其作用是防止个别异常顶点的出现。（边缘损失）</li></ol> <h2 id="_4-实验"><a href="#_4-实验" class="header-anchor">#</a> 4 实验</h2> <h3 id="_4-1-实验设置"><a href="#_4-1-实验设置" class="header-anchor">#</a> 4.1 实验设置</h3> <p>数据，评价指标，对比标准，训练和测试，...</p> <h3 id="_4-2-与最新技术的对比"><a href="#_4-2-与最新技术的对比" class="header-anchor">#</a> 4.2 与最新技术的对比</h3> <h3 id="_4-3-消融实验"><a href="#_4-3-消融实验" class="header-anchor">#</a> 4.3 消融实验</h3> <p>消融研究的定性结果。该图真实地反映了各组成部分的贡献，尤其是正则化组成部分。</p> <p><img src="/assets/img/image-20211213171750441.ca4fc7d6.png" alt="image-20211213171750441"></p></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/paper/期刊文章/03 利用基元分解的机载点云复杂建筑物自动重建.html" class="prev">
        利用基元分解的机载点云复杂建筑物自动重建
      </a></span> <span class="next"><a href="/paper/期刊文章/05 OpenDR An ApproximateDiﬀerentiable Renderer.html">
        Open DR: An Approximate Diﬀerentiable  Renderer
      </a>
      →
    </span></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/assets/js/app.4da1065c.js" defer></script><script src="/assets/js/3.d419b5b7.js" defer></script><script src="/assets/js/21.ffea2103.js" defer></script>
  </body>
</html>
